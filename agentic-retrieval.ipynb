{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4be5d807",
   "metadata": {},
   "source": [
    "# Quickstart: Agentic retrieval in Azure AI Search\n",
    "\n",
    "Use this notebook to get started with [agentic retrieval](https://learn.microsoft.com/azure/search/search-agentic-retrieval-concept) in Azure AI Search, which integrates an Azure OpenAI chat completion model to process queries, retrieve relevant content from indexed documents, and generate natural-language answers.\n",
    "\n",
    "Steps in this notebook include:\n",
    "\n",
    "1. Creating and loading an `earth-at-night` search index.\n",
    "\n",
    "1. Creating an `earth-knowledge-source` that targets your index.\n",
    "\n",
    "1. Creating an `earth-knowledge-agent` that targets your knowledge source and an LLM for query planning and answer synthesis.\n",
    "\n",
    "1. Using the agent to fetch, rank, and synthesize relevant information from the index.\n",
    "\n",
    "This notebook provides a high-level demonstration of agentic retrieval. For more detailed guidance, see [Quickstart: Run agentic retrieval in Azure AI Search](https://learn.microsoft.com/azure/search/search-get-started-agentic-retrieval)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712b97d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "+ An [Azure AI Search service](https://learn.microsoft.com/azure/search/search-create-service-portal) on the Basic tier or higher with [semantic ranker enabled](https://learn.microsoft.com/azure/search/semantic-how-to-enable-disable).\n",
    "\n",
    "+ An [Azure AI Foundry project](https://learn.microsoft.com/azure/ai-foundry/how-to/create-projects) and Azure AI Foundry resource. When you create a project, the resource is automatically created.\n",
    "\n",
    "+ A [supported chat completion model](https://learn.microsoft.com/azure/search/search-agentic-retrieval-how-to-create#supported-models). This sample uses `gpt-5-mini`.\n",
    "\n",
    "+ A text embedding model. This sample uses `text-embedding-3-large`.\n",
    "\n",
    "+ [Visual Studio Code](https://code.visualstudio.com/download) with the [Python extension](https://marketplace.visualstudio.com/items?itemName=ms-python.python) and [Jupyter package](https://pypi.org/project/jupyter/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5fbd46",
   "metadata": {},
   "source": [
    "## Configure access\n",
    "\n",
    "This notebook assumes that you're using Microsoft Entra ID for authentication and role assignments for authorization.\n",
    "\n",
    "To configure role-based access:\n",
    "\n",
    "1. Sign in to the [Azure portal](https://portal.azure.com).\n",
    "\n",
    "1. On your Azure AI Search service:\n",
    "\n",
    "    1. [Enable role-based access](https://learn.microsoft.com/azure/search/search-security-enable-roles).\n",
    "    \n",
    "    1. [Create a system-assigned managed identity](https://learn.microsoft.com/azure/search/search-howto-managed-identities-data-sources#create-a-system-managed-identity).\n",
    "    \n",
    "    1. [Assign the following roles](https://learn.microsoft.com/azure/search/search-security-rbac#how-to-assign-roles-in-the-azure-portal) to yourself.\n",
    "    \n",
    "       + **Search Service Contributor**\n",
    "    \n",
    "       + **Search Index Data Contributor**\n",
    "    \n",
    "       + **Search Index Data Reader**\n",
    "\n",
    "1. On your Azure AI Foundry resource, assign **Cognitive Services User** to the managed identity of your search service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733bf308",
   "metadata": {},
   "source": [
    "## Set up connections\n",
    "\n",
    "The `sample.env` file contains environment variables for connections to Azure AI Search and Azure OpenAI in Azure AI Foundry. Agentic retrieval requires these connections for document retrieval, query planning, and query execution.\n",
    "\n",
    "To set up the connections:\n",
    "\n",
    "1. Sign in to the [Azure portal](https://portal.azure.com).\n",
    "\n",
    "1. Get the endpoints for Azure AI Search (`https://your-search-service.search.windows.net`) and Azure OpenAI in Azure AI Foundry (`https://your-foundry-resource.openai.azure.com`).\n",
    "\n",
    "1. Save the `sample.env` file as `.env` on your local system.\n",
    "\n",
    "1. Update the `.env` file with the retrieved endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a54a0f",
   "metadata": {},
   "source": [
    "## Create a virtual environment\n",
    "\n",
    "The `requirements.txt` file contains the dependencies for this notebook. You can use a virtual environment to install these dependencies in isolation.\n",
    "\n",
    "To create a virtual environment:\n",
    "\n",
    "1. In Visual Studio Code, open the folder that contains `quickstart-agentic-retrieval.ipynb`.\n",
    "\n",
    "1. Press **Ctrl**+**Shift**+**P** to open the command palette.\n",
    "\n",
    "1. Search for **Python: Create Environment**, and then select **Venv**.\n",
    "\n",
    "1. Select a Python installation. We tested this notebook on Python 3.13.7.\n",
    "\n",
    "1. Select `requirements.txt` for the dependencies.\n",
    "\n",
    "Creating the virtual environment can take several minutes. When the environment is ready, proceed to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0714a968",
   "metadata": {},
   "source": [
    "## Install packages and load connections\n",
    "\n",
    "This step installs the packages for this notebook and establishes connections to Azure AI Search and Azure OpenAI in Azure AI Foundry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041e5d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df3a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "import os\n",
    "\n",
    "# Take environment variables from .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# This notebook uses the following variables from your .env file\n",
    "search_endpoint = os.environ[\"SEARCH_ENDPOINT\"]\n",
    "credential = DefaultAzureCredential()  # Keep this for Search service\n",
    "aoai_endpoint = os.environ[\"AOAI_ENDPOINT\"]\n",
    "aoai_api_key = os.environ[\"AOAI_API_KEY\"]  # Add this line\n",
    "aoai_embedding_model = os.environ[\"AOAI_EMBEDDING_MODEL\"]\n",
    "aoai_embedding_deployment = os.environ[\"AOAI_EMBEDDING_DEPLOYMENT\"]\n",
    "aoai_gpt_model = os.environ[\"AOAI_GPT_MODEL\"]\n",
    "aoai_gpt_deployment = os.environ[\"AOAI_GPT_DEPLOYMENT\"]\n",
    "index_name = os.environ[\"INDEX_NAME\"]\n",
    "knowledge_source_name = os.environ[\"KNOWLEDGE_SOURCE_NAME\"]\n",
    "knowledge_agent_name = os.environ[\"KNOWLEDGE_AGENT_NAME\"]\n",
    "search_api_version = os.environ[\"SEARCH_API_VERSION\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8a088",
   "metadata": {},
   "source": [
    "## Create a search index\n",
    "\n",
    "This step creates an index that contains plain text and vector content. You can use an existing index, but it must meet the criteria for [agentic retrieval workloads](https://learn.microsoft.com/azure/search/search-agentic-retrieval-how-to-index). The primary schema requirement is a semantic configuration with a `default_configuration_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ee48bec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "retrievable is not a known attribute of class <class 'azure.search.documents.indexes.models._index.SearchField'> and will be ignored\n",
      "retrievable is not a known attribute of class <class 'azure.search.documents.indexes.models._index.SearchField'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'earth-at-night' created or updated successfully.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import SearchIndex, SearchField, VectorSearch, VectorSearchProfile, HnswAlgorithmConfiguration, AzureOpenAIVectorizer, AzureOpenAIVectorizerParameters, SemanticSearch, SemanticConfiguration, SemanticPrioritizedFields, SemanticField\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load API keys from environment\n",
    "aoai_api_key = os.environ[\"AOAI_API_KEY\"]\n",
    "search_api_key = os.environ[\"SEARCH_API_KEY\"]\n",
    "\n",
    "# Create search credential using API key\n",
    "search_credential = AzureKeyCredential(search_api_key)\n",
    "\n",
    "index = SearchIndex(\n",
    "    name=index_name,\n",
    "    fields=[\n",
    "        # Original fields\n",
    "        SearchField(name=\"id\", type=\"Edm.String\", key=True, filterable=True, sortable=True, facetable=True),\n",
    "        SearchField(name=\"page_chunk\", type=\"Edm.String\", filterable=False, sortable=False, facetable=False),\n",
    "        SearchField(name=\"page_embedding_text_3_large\", type=\"Collection(Edm.Single)\", stored=False, vector_search_dimensions=3072, vector_search_profile_name=\"hnsw_text_3_large\"),\n",
    "        SearchField(name=\"page_number\", type=\"Edm.Int32\", filterable=True, sortable=True, facetable=True),\n",
    "        \n",
    "        # Video metadata fields\n",
    "        SearchField(name=\"content_type\", type=\"Edm.String\", filterable=True, facetable=True),  # \"document\" or \"video\"\n",
    "        SearchField(name=\"video_id\", type=\"Edm.String\", filterable=True, sortable=True, facetable=True),\n",
    "        SearchField(name=\"title\", type=\"Edm.String\", searchable=True, filterable=True, sortable=True),\n",
    "        SearchField(name=\"description\", type=\"Edm.String\", searchable=True, filterable=False),\n",
    "        SearchField(name=\"duration\", type=\"Edm.Double\", filterable=True, sortable=True, facetable=True),\n",
    "        SearchField(name=\"category\", type=\"Edm.String\", filterable=True, facetable=True, searchable=True),\n",
    "        SearchField(name=\"tags\", type=\"Collection(Edm.String)\", searchable=True, filterable=True),\n",
    "        SearchField(name=\"instructor\", type=\"Edm.String\", filterable=True, facetable=True, searchable=True),\n",
    "        SearchField(name=\"upload_date\", type=\"Edm.DateTimeOffset\", filterable=True, sortable=True),\n",
    "        SearchField(name=\"thumbnail_url\", type=\"Edm.String\", retrievable=True, stored=True),\n",
    "        SearchField(name=\"video_url\", type=\"Edm.String\", retrievable=True, stored=True),\n",
    "        SearchField(name=\"transcript\", type=\"Edm.String\", searchable=True, filterable=False),  # For video transcripts if available\n",
    "        SearchField(name=\"level\", type=\"Edm.String\", filterable=True, facetable=True)  # beginner, intermediate, advanced\n",
    "    ],\n",
    "    vector_search=VectorSearch(\n",
    "        profiles=[VectorSearchProfile(name=\"hnsw_text_3_large\", algorithm_configuration_name=\"alg\", vectorizer_name=\"text-embedding-3-large\")],\n",
    "        algorithms=[HnswAlgorithmConfiguration(name=\"alg\")],\n",
    "        vectorizers=[\n",
    "            AzureOpenAIVectorizer(\n",
    "                vectorizer_name=\"text-embedding-3-large\",\n",
    "                parameters=AzureOpenAIVectorizerParameters(\n",
    "                    resource_url=aoai_endpoint,\n",
    "                    deployment_name=aoai_embedding_deployment,\n",
    "                    model_name=aoai_embedding_model,\n",
    "                    api_key=aoai_api_key\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    semantic_search=SemanticSearch(\n",
    "        default_configuration_name=\"semantic_config\",\n",
    "        configurations=[\n",
    "            SemanticConfiguration(\n",
    "                name=\"semantic_config\",\n",
    "                prioritized_fields=SemanticPrioritizedFields(\n",
    "                    title_field=SemanticField(field_name=\"title\"),  # Added title field for semantic search\n",
    "                    content_fields=[\n",
    "                        SemanticField(field_name=\"page_chunk\"),\n",
    "                        SemanticField(field_name=\"description\"),  # Added video descriptions\n",
    "                        SemanticField(field_name=\"transcript\")   # Added video transcripts\n",
    "                    ],\n",
    "                    keywords_fields=[\n",
    "                        SemanticField(field_name=\"tags\"),        # Added video tags\n",
    "                        SemanticField(field_name=\"category\"),    # Added video categories\n",
    "                        SemanticField(field_name=\"video_url\")   # Added video URLs\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Use API key credential instead of managed identity\n",
    "index_client = SearchIndexClient(endpoint=search_endpoint, credential=search_credential)\n",
    "index_client.create_or_update_index(index)\n",
    "print(f\"Index '{index_name}' created or updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b948b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the existing indexer first\n",
    "indexer_client.delete_indexer(\"training-videos-indexer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "206317cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexer updated with content_type field\n"
     ]
    }
   ],
   "source": [
    "# Delete and recreate indexer with content_type properly set\n",
    "indexer_client.delete_indexer(\"training-videos-indexer\")\n",
    "\n",
    "video_indexer = SearchIndexer(\n",
    "    name=\"training-videos-indexer\",\n",
    "    data_source_name=\"training-videos-datasource\", \n",
    "    target_index_name=index_name,\n",
    "    field_mappings=[\n",
    "        FieldMapping(source_field_name=\"video_id\", target_field_name=\"id\"),\n",
    "        FieldMapping(source_field_name=\"title\", target_field_name=\"title\"),\n",
    "        FieldMapping(source_field_name=\"description\", target_field_name=\"description\"),\n",
    "        #FieldMapping(source_field_name=\"video_id\", target_field_name=\"video_id\"),\n",
    "        FieldMapping(source_field_name=\"category\", target_field_name=\"category\"),\n",
    "        FieldMapping(source_field_name=\"duration_seconds\", target_field_name=\"duration\"),\n",
    "        FieldMapping(source_field_name=\"tags\", target_field_name=\"tags\"),\n",
    "        FieldMapping(source_field_name=\"instructor\", target_field_name=\"instructor\"),\n",
    "        FieldMapping(source_field_name=\"created_date\", target_field_name=\"upload_date\"),\n",
    "        FieldMapping(source_field_name=\"thumbnail_url\", target_field_name=\"thumbnail_url\"),\n",
    "        FieldMapping(source_field_name=\"video_url\", target_field_name=\"video_url\"),\n",
    "        FieldMapping(source_field_name=\"transcript\", target_field_name=\"page_chunk\"),\n",
    "        #FieldMapping(source_field_name=\"difficulty_level\", target_field_name=\"level\"),\n",
    "        # Add explicit content_type mapping\n",
    "        #FieldMapping(source_field_name=\"/document/content_type\", target_field_name=\"content_type\", target_field_value=\"video\"),\n",
    "    ],\n",
    "    parameters={\n",
    "        \"configuration\": {\n",
    "            \"dataToExtract\": \"contentAndMetadata\",\n",
    "            \"parsingMode\": \"json\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "indexer_client.create_or_update_indexer(video_indexer)\n",
    "indexer_client.run_indexer(\"training-videos-indexer\")\n",
    "print(\"Indexer updated with content_type field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3c9f773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob data source created successfully.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import SearchIndexerDataSourceConnection, SearchIndexerDataContainer\n",
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "\n",
    "# Load blob connection string from environment\n",
    "blob_connection_string = os.environ[\"BLOB_CONNECTION_STRING\"]\n",
    "\n",
    "# Create SearchIndexerClient (not SearchIndexClient)\n",
    "indexer_client = SearchIndexerClient(endpoint=search_endpoint, credential=search_credential)\n",
    "\n",
    "# Create data source for your blob storage\n",
    "blob_data_source = SearchIndexerDataSourceConnection(\n",
    "    name=\"training-videos-datasource\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=blob_connection_string,\n",
    "    container=SearchIndexerDataContainer(name=\"training-videos\", query=\"metadata\")\n",
    ")\n",
    "\n",
    "indexer_client.create_or_update_data_source_connection(blob_data_source)\n",
    "print(\"Blob data source created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39874f61",
   "metadata": {},
   "source": [
    "## Upload sample documents\n",
    "\n",
    "This notebook uses data from NASA's Earth at Night e-book. The data is retrieved from the [azure-search-sample-data](https://github.com/Azure-Samples/azure-search-sample-data) repository on GitHub and passed to the search client for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ded5147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents uploaded to index 'earth-at-night' successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Azure-Samples/azure-search-sample-data/refs/heads/main/nasa-e-book/earth-at-night-json/documents.json\"\n",
    "documents = requests.get(url).json()\n",
    "\n",
    "# Use the same search_credential that you created earlier\n",
    "with SearchIndexingBufferedSender(endpoint=search_endpoint, index_name=index_name, credential=search_credential) as client:\n",
    "    client.upload_documents(documents=documents)\n",
    "\n",
    "print(f\"Documents uploaded to index '{index_name}' successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "99901720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video metadata indexer created successfully.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import SearchIndexer, FieldMapping\n",
    "\n",
    "# Create indexer to process your JSON metadata files\n",
    "video_indexer = SearchIndexer(\n",
    "    name=\"training-videos-indexer\",\n",
    "    data_source_name=\"training-videos-datasource\",\n",
    "    target_index_name=index_name,\n",
    "    field_mappings=[\n",
    "        FieldMapping(source_field_name=\"metadata_storage_name\", target_field_name=\"id\"),\n",
    "        FieldMapping(source_field_name=\"content\", target_field_name=\"page_chunk\"),  # JSON content as searchable text\n",
    "    ],\n",
    "    parameters={\n",
    "        \"configuration\": {\n",
    "            \"dataToExtract\": \"contentAndMetadata\",\n",
    "            \"parsingMode\": \"json\",\n",
    "            \"documentRoot\": \"$\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "indexer_client.create_or_update_indexer(video_indexer)\n",
    "print(\"Video metadata indexer created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb9e5f",
   "metadata": {},
   "source": [
    "## Create a knowledge source\n",
    "\n",
    "This step creates a knowledge source that targets the index you previously created. In the next step, you create a knowledge agent that uses the knowledge source to orchestrate agentic retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e3415954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge source 'earth-knowledge-source' created or updated successfully.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import SearchIndexKnowledgeSource, SearchIndexKnowledgeSourceParameters\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "\n",
    "ks = SearchIndexKnowledgeSource(\n",
    "    name=knowledge_source_name,\n",
    "    description=\"Knowledge source for Earth at night data and training videos\",\n",
    "    search_index_parameters=SearchIndexKnowledgeSourceParameters(\n",
    "        search_index_name=index_name,\n",
    "        source_data_select=\"id,page_chunk,page_number,content_type,video_id,title,description,duration,category,tags,instructor,upload_date,thumbnail_url,video_url\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Use the same search_credential with API key\n",
    "index_client = SearchIndexClient(endpoint=search_endpoint, credential=search_credential)\n",
    "index_client.create_or_update_knowledge_source(knowledge_source=ks, api_version=search_api_version)\n",
    "print(f\"Knowledge source '{knowledge_source_name}' created or updated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5e0a34",
   "metadata": {},
   "source": [
    "## Create a knowledge agent\n",
    "\n",
    "This step creates a knowledge agent, which acts as a wrapper for your knowledge source and LLM deployment.\n",
    "\n",
    "`EXTRACTIVE_DATA` is the default modality and returns content from your knowledge sources without generative alteration. However, this quickstart uses the `ANSWER_SYNTHESIS` modality for LLM-generated answers that cite the retrieved content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d3fe4183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge agent 'earth-knowledge-agent' created or updated successfully.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import KnowledgeAgent, KnowledgeAgentAzureOpenAIModel, KnowledgeSourceReference, AzureOpenAIVectorizerParameters, KnowledgeAgentOutputConfiguration, KnowledgeAgentOutputConfigurationModality\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "\n",
    "aoai_params = AzureOpenAIVectorizerParameters(\n",
    "    resource_url=aoai_endpoint,\n",
    "    deployment_name=aoai_gpt_deployment,\n",
    "    model_name=aoai_gpt_model,\n",
    "    api_key=aoai_api_key  # Add the API key\n",
    ")\n",
    "\n",
    "output_cfg = KnowledgeAgentOutputConfiguration(\n",
    "    modality=KnowledgeAgentOutputConfigurationModality.ANSWER_SYNTHESIS,\n",
    "    include_activity=True,\n",
    ")\n",
    "\n",
    "agent = KnowledgeAgent(\n",
    "    name=knowledge_agent_name,\n",
    "    models=[KnowledgeAgentAzureOpenAIModel(azure_open_ai_parameters=aoai_params)],\n",
    "    knowledge_sources=[\n",
    "        KnowledgeSourceReference(\n",
    "            name=knowledge_source_name,\n",
    "            reranker_threshold=2.5,\n",
    "        )\n",
    "    ],\n",
    "    output_configuration=output_cfg,\n",
    ")\n",
    "\n",
    "# Use the same search_credential with API key\n",
    "index_client = SearchIndexClient(endpoint=search_endpoint, credential=search_credential)\n",
    "index_client.create_or_update_agent(agent, api_version=search_api_version)\n",
    "print(f\"Knowledge agent '{knowledge_agent_name}' created or updated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1610cefd",
   "metadata": {},
   "source": [
    "## Set up messages\n",
    "\n",
    "Messages are the input for the retrieval route and contain the conversation history. Each message includes a `role` that indicates its origin, such as `system` or `user`, and `content` in natural language. The LLM you use determines which roles are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2ab7b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are a Q&A agent for Earth at night data and training videos.\n",
    "\n",
    "CRITICAL: When users ask for training videos, you MUST return:\n",
    "- Video title\n",
    "- Complete video_url (the full https:// link from the video_url field)\n",
    "- Instructor name\n",
    "- Duration\n",
    "\n",
    "Format video responses like this:\n",
    "Title: [title]\n",
    "Video URL: [video_url]\n",
    "Instructor: [instructor]\n",
    "Duration: [duration] seconds\n",
    "\n",
    "You have access to video_url, instructor, duration, and other metadata fields. USE THEM.\n",
    "Never just return the title - always include the complete video_url field value.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4090707f",
   "metadata": {},
   "source": [
    "## Use agentic retrieval to fetch results\n",
    "\n",
    "This step runs the agentic retrieval pipeline to produce a grounded, citation-backed answer. Given the conversation history and retrieval parameters, your knowledge agent:\n",
    "\n",
    "1. Analyzes the entire conversation to infer the user's information need.\n",
    "\n",
    "1. Decomposes the compound query into focused subqueries.\n",
    "\n",
    "1. Runs the subqueries concurrently against your knowledge source.\n",
    "\n",
    "1. Uses semantic ranker to rerank and filter the results.\n",
    "\n",
    "1. Synthesizes the top results into a natural-language answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "918ded26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved content from 'earth-knowledge-source' successfully.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.agent import KnowledgeAgentRetrievalClient\n",
    "from azure.search.documents.agent.models import KnowledgeAgentRetrievalRequest, KnowledgeAgentMessage, KnowledgeAgentMessageTextContent, SearchIndexKnowledgeSourceParams\n",
    "\n",
    "# Use search_credential instead of credential\n",
    "agent_client = KnowledgeAgentRetrievalClient(endpoint=search_endpoint, agent_name=knowledge_agent_name, credential=search_credential)\n",
    "query_1 = \"\"\"\n",
    "    Why do suburban belts display larger December brightening than urban cores even though absolute light levels are higher downtown?\n",
    "    Why is the Phoenix nighttime street grid is so sharply visible from space, whereas large stretches of the interstate between midwestern cities remain comparatively dim?\n",
    "    \"\"\"\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": query_1\n",
    "})\n",
    "\n",
    "req = KnowledgeAgentRetrievalRequest(\n",
    "    messages=[\n",
    "        KnowledgeAgentMessage(\n",
    "            role=m[\"role\"],\n",
    "            content=[KnowledgeAgentMessageTextContent(text=m[\"content\"])]\n",
    "        ) for m in messages if m[\"role\"] != \"system\"\n",
    "    ],\n",
    "    knowledge_source_params=[\n",
    "        SearchIndexKnowledgeSourceParams(\n",
    "            knowledge_source_name=knowledge_source_name,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = agent_client.retrieve(retrieval_request=req, api_version=search_api_version)\n",
    "print(f\"Retrieved content from '{knowledge_source_name}' successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6fb279a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved video content from 'earth-knowledge-source' successfully.\n"
     ]
    }
   ],
   "source": [
    "# Add a video-related query to test video search functionality\n",
    "query_2 = \"\"\"\n",
    "    What is the video_url field value for the Intermediate training video? I need the complete https:// link.\"\n",
    "    \"\"\"\n",
    "\n",
    "# Add the video query to messages\n",
    "messages.append({\n",
    "    \"role\": \"user\", \n",
    "    \"content\": query_2\n",
    "})\n",
    "\n",
    "# Create request for the video query\n",
    "req_2 = KnowledgeAgentRetrievalRequest(\n",
    "    messages=[\n",
    "        KnowledgeAgentMessage(\n",
    "            role=m[\"role\"],\n",
    "            content=[KnowledgeAgentMessageTextContent(text=m[\"content\"])]\n",
    "        ) for m in messages if m[\"role\"] != \"system\"\n",
    "    ],\n",
    "    knowledge_source_params=[\n",
    "        SearchIndexKnowledgeSourceParams(\n",
    "            knowledge_source_name=knowledge_source_name,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Execute the video query\n",
    "result_2 = agent_client.retrieve(retrieval_request=req_2, api_version=search_api_version)\n",
    "print(f\"Retrieved video content from '{knowledge_source_name}' successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886fc687",
   "metadata": {},
   "source": [
    "### Review the retrieval response, activity, and results\n",
    "\n",
    "Because your knowledge agent is configured for answer synthesis, the retrieval response contains the following values:\n",
    "\n",
    "+ `response_content`: An LLM-generated answer to the query that cites the retrieved documents.\n",
    "\n",
    "+ `activity_content`: Detailed planning and execution information, including subqueries, reranking decisions, and intermediate steps.\n",
    "\n",
    "+ `references_content`: Source documents and chunks that contributed to the answer.\n",
    "\n",
    "**Tip:** Retrieval parameters, such as reranker thresholds and knowledge source parameters, influence how aggressively your agent reranks and which sources it queries. Inspect the activity and references to validate grounding and build traceable citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d4d78fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_contents = []\n",
    "activity_contents = []\n",
    "references_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7fccf4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Response Content 1 ===\n",
      "response_content:\n",
      " To find training content for advanced learners, you can access the advanced-level training video titled \"Training Video\". This course provides comprehensive instruction on the topic and is available at the following link: https://mariagana.blob.core.windows.net/training-videos/Advanced.mp4 [ref_id:0]. \n",
      "\n",
      "=== Response Content 2 ===\n",
      "response_content:\n",
      " The video URL for the Intermediate training video is https://mariagana.blob.core.windows.net/training-videos/Intermediate.mp4 [ref_id:0]. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Build simple string values for response_content, activity_content, and references_content\n",
    "\n",
    "# Process both results\n",
    "for i, result_obj in enumerate([result, result_2], 1):\n",
    "    # Responses -> Concatenate text/value fields from all response contents\n",
    "    response_parts = []\n",
    "    if getattr(result_obj, \"response\", None):\n",
    "        for resp in result_obj.response:\n",
    "            for content in getattr(resp, \"content\", []):\n",
    "                text = getattr(content, \"text\", None) or getattr(content, \"value\", None) or str(content)\n",
    "                response_parts.append(text)\n",
    "    \n",
    "    response_content = \"\\n\\n\".join(response_parts) if response_parts else f\"No response found on 'result_{i}'\"\n",
    "    response_contents.append(response_content)\n",
    "    \n",
    "    # Print the response content\n",
    "    print(f\"=== Response Content {i} ===\")\n",
    "    print(\"response_content:\\n\", response_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7355941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": response_content\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4cef4fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity_content:\n",
      " [\n",
      "  {\n",
      "    \"id\": 0,\n",
      "    \"type\": \"modelQueryPlanning\",\n",
      "    \"elapsed_ms\": 1328,\n",
      "    \"input_tokens\": 2074,\n",
      "    \"output_tokens\": 146\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"type\": \"searchIndex\",\n",
      "    \"elapsed_ms\": 242,\n",
      "    \"knowledge_source_name\": \"earth-knowledge-source\",\n",
      "    \"query_time\": \"2025-10-22T06:15:33.379Z\",\n",
      "    \"count\": 0,\n",
      "    \"search_index_arguments\": {\n",
      "      \"search\": \"Why do suburban belts display larger December brightening than urban cores?\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"type\": \"searchIndex\",\n",
      "    \"elapsed_ms\": 230,\n",
      "    \"knowledge_source_name\": \"earth-knowledge-source\",\n",
      "    \"query_time\": \"2025-10-22T06:15:33.610Z\",\n",
      "    \"count\": 0,\n",
      "    \"search_index_arguments\": {\n",
      "      \"search\": \"Why are absolute light levels higher downtown?\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"type\": \"searchIndex\",\n",
      "    \"elapsed_ms\": 167,\n",
      "    \"knowledge_source_name\": \"earth-knowledge-source\",\n",
      "    \"query_time\": \"2025-10-22T06:15:33.777Z\",\n",
      "    \"count\": 1,\n",
      "    \"search_index_arguments\": {\n",
      "      \"search\": \"Why is the Phoenix nighttime street grid sharply visible from space?\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": 4,\n",
      "    \"type\": \"semanticReranker\",\n",
      "    \"input_tokens\": 70348\n",
      "  },\n",
      "  {\n",
      "    \"id\": 5,\n",
      "    \"type\": \"modelAnswerSynthesis\",\n",
      "    \"elapsed_ms\": 2869,\n",
      "    \"input_tokens\": 2815,\n",
      "    \"output_tokens\": 289\n",
      "  }\n",
      "] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Activity -> JSON string of activity as list of dicts\n",
    "if getattr(result, \"activity\", None):\n",
    "    activity_content = json.dumps([a.as_dict() for a in result.activity], indent=2)\n",
    "else:\n",
    "    activity_content = \"No activity found on 'result'\"\n",
    "    \n",
    "activity_contents.append(activity_content)\n",
    "print(\"activity_content:\\n\", activity_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "172df234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "references_content:\n",
      " [\n",
      "  {\n",
      "    \"type\": \"searchIndex\",\n",
      "    \"id\": \"0\",\n",
      "    \"activity_source\": 3,\n",
      "    \"reranker_score\": 2.7294974,\n",
      "    \"doc_key\": \"earth_at_night_508_page_104_verbalized\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# References -> JSON string of references as list of dicts\n",
    "if getattr(result, \"references\", None):\n",
    "    references_content = json.dumps([r.as_dict() for r in result.references], indent=2)\n",
    "else:\n",
    "    references_content = \"No references found on 'result'\"\n",
    "    \n",
    "references_contents.append(references_content)\n",
    "print(\"references_content:\\n\", references_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75386ed1",
   "metadata": {},
   "source": [
    "## Continue the conversation\n",
    "\n",
    "This step continues the conversation with your knowledge agent, building upon the previous messages and queries to retrieve relevant information from your knowledge source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2b674f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "A Q&A agent that can answer questions about the Earth at night and training videos. \n",
    "When recommending training videos, ALWAYS include the complete video URL, instructor name, and duration.\n",
    "For video queries, provide the direct link in this format: \"Video URL: [full URL]\"\n",
    "If you don't have the answer, respond with \"I don't know\".\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": instructions\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "da260539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved content from 'earth-knowledge-source' successfully.\n"
     ]
    }
   ],
   "source": [
    "query_2 = \"How do I find training content for Advanced learners?\"\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": query_2\n",
    "})\n",
    "\n",
    "req = KnowledgeAgentRetrievalRequest(\n",
    "    messages=[\n",
    "        KnowledgeAgentMessage(\n",
    "            role=m[\"role\"],\n",
    "            content=[KnowledgeAgentMessageTextContent(text=m[\"content\"])]\n",
    "        ) for m in messages if m[\"role\"] != \"system\"\n",
    "    ],\n",
    "    knowledge_source_params=[\n",
    "        SearchIndexKnowledgeSourceParams(\n",
    "            knowledge_source_name=knowledge_source_name,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = agent_client.retrieve(retrieval_request=req, api_version=search_api_version)\n",
    "print(f\"Retrieved content from '{knowledge_source_name}' successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1cba0c",
   "metadata": {},
   "source": [
    "### Review the new retrieval response, activity, and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "35a1bfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_content:\n",
      " To find training content for advanced learners, you can access the advanced-level training video titled \"Training Video\". This course provides comprehensive instruction on the topic and is available at the following link: https://mariagana.blob.core.windows.net/training-videos/Advanced.mp4 [ref_id:0]. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Build simple string values for response_content, activity_content, and references_content\n",
    "\n",
    "# Responses -> Concatenate text/value fields from all response contents\n",
    "response_parts = []\n",
    "if getattr(result, \"response\", None):\n",
    "    for resp in result.response:\n",
    "        for content in getattr(resp, \"content\", []):\n",
    "            text = getattr(content, \"text\", None) or getattr(content, \"value\", None) or str(content)\n",
    "            response_parts.append(text)\n",
    "response_content = \"\\n\\n\".join(response_parts) if response_parts else \"No response found on 'result'\"\n",
    "\n",
    "response_contents.append(response_content)\n",
    "\n",
    "# Print the three string values\n",
    "print(\"response_content:\\n\", response_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "40fe9cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved combined content successfully.\n"
     ]
    }
   ],
   "source": [
    "# Combined query that spans both Earth science documents and training videos\n",
    "query_combined = \"\"\"\n",
    "Any training videos available for beginners, any training videos would be fine. \n",
    "Please provide the video URLs and instructor information for any relevant courses.\n",
    "\"\"\"\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": query_combined\n",
    "})\n",
    "\n",
    "req_combined = KnowledgeAgentRetrievalRequest(\n",
    "    messages=[\n",
    "        KnowledgeAgentMessage(\n",
    "            role=m[\"role\"],\n",
    "            content=[KnowledgeAgentMessageTextContent(text=m[\"content\"])]\n",
    "        ) for m in messages if m[\"role\"] != \"system\"\n",
    "    ],\n",
    "    knowledge_source_params=[\n",
    "        SearchIndexKnowledgeSourceParams(\n",
    "            knowledge_source_name=knowledge_source_name,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "result_combined = agent_client.retrieve(retrieval_request=req_combined, api_version=search_api_version)\n",
    "print(\"Retrieved combined content successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6f74c2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity_content:\n",
      " [\n",
      "  {\n",
      "    \"id\": 0,\n",
      "    \"type\": \"modelQueryPlanning\",\n",
      "    \"elapsed_ms\": 1257,\n",
      "    \"input_tokens\": 2188,\n",
      "    \"output_tokens\": 79\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"type\": \"searchIndex\",\n",
      "    \"elapsed_ms\": 427,\n",
      "    \"knowledge_source_name\": \"earth-knowledge-source\",\n",
      "    \"query_time\": \"2025-10-22T06:48:43.426Z\",\n",
      "    \"count\": 1,\n",
      "    \"search_index_arguments\": {\n",
      "      \"search\": \"advanced training content for learners\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"type\": \"searchIndex\",\n",
      "    \"elapsed_ms\": 162,\n",
      "    \"knowledge_source_name\": \"earth-knowledge-source\",\n",
      "    \"query_time\": \"2025-10-22T06:48:43.588Z\",\n",
      "    \"count\": 0,\n",
      "    \"search_index_arguments\": {\n",
      "      \"search\": \"training resources for advanced learners\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"type\": \"searchIndex\",\n",
      "    \"elapsed_ms\": 372,\n",
      "    \"knowledge_source_name\": \"earth-knowledge-source\",\n",
      "    \"query_time\": \"2025-10-22T06:48:43.960Z\",\n",
      "    \"count\": 0,\n",
      "    \"search_index_arguments\": {\n",
      "      \"search\": \"advanced learning materials\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": 4,\n",
      "    \"type\": \"semanticReranker\",\n",
      "    \"input_tokens\": 65374\n",
      "  },\n",
      "  {\n",
      "    \"id\": 5,\n",
      "    \"type\": \"modelAnswerSynthesis\",\n",
      "    \"elapsed_ms\": 738,\n",
      "    \"input_tokens\": 2392,\n",
      "    \"output_tokens\": 67\n",
      "  }\n",
      "] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Activity -> JSON string of activity as list of dicts\n",
    "if getattr(result, \"activity\", None):\n",
    "    activity_content = json.dumps([a.as_dict() for a in result.activity], indent=2)\n",
    "else:\n",
    "    activity_content = \"No activity found on 'result'\"\n",
    "    \n",
    "activity_contents.append(activity_content)\n",
    "print(\"activity_content:\\n\", activity_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "22203bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Combined Query Response ===\n",
      "There is a training video available for beginners titled \"Introduction.\" This course provides comprehensive instruction on the topic at a beginner level. You can access the training video through the following URL: https://mariagana.blob.core.windows.net/training-videos/Introduction.mp4 [ref_id:0]. No specific instructor information was found in the retrieved content.\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Task Decomposition Activity ===\n",
      "Step 1: modelQueryPlanning\n",
      "  Time: 1334ms\n",
      "\n",
      "Step 2: searchIndex\n",
      "  Search Query: beginner training videos for urban lighting analysis\n",
      "  Results Found: 0\n",
      "  Time: 496ms\n",
      "\n",
      "Step 3: searchIndex\n",
      "  Search Query: video URLs for beginner training courses\n",
      "  Results Found: 1\n",
      "  Time: 420ms\n",
      "\n",
      "Step 4: searchIndex\n",
      "  Search Query: instructor information for beginner courses on urban lighting\n",
      "  Results Found: 0\n",
      "  Time: 405ms\n",
      "\n",
      "Step 5: semanticReranker\n",
      "\n",
      "Step 6: modelAnswerSynthesis\n",
      "  Time: 961ms\n",
      "\n",
      "=== References Used ===\n",
      "Reference 1:\n",
      "  Document Key: beginner-introduction\n",
      "  Reranker Score: 2.5109673\n",
      "  Activity Source: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract and display the response content\n",
    "response_parts = []\n",
    "if getattr(result_combined, \"response\", None):\n",
    "    for resp in result_combined.response:\n",
    "        for content in getattr(resp, \"content\", []):\n",
    "            text = getattr(content, \"text\", None) or getattr(content, \"value\", None) or str(content)\n",
    "            response_parts.append(text)\n",
    "\n",
    "response_content = \"\\n\\n\".join(response_parts) if response_parts else \"No response found\"\n",
    "print(\"=== Combined Query Response ===\")\n",
    "print(response_content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Display activity logs to see task decomposition\n",
    "if getattr(result_combined, \"activity\", None):\n",
    "    print(\"=== Task Decomposition Activity ===\")\n",
    "    for i, activity in enumerate(result_combined.activity):\n",
    "        activity_dict = activity.as_dict()\n",
    "        print(f\"Step {i+1}: {activity_dict.get('type', 'Unknown')}\")\n",
    "        if 'search_index_arguments' in activity_dict:\n",
    "            print(f\"  Search Query: {activity_dict['search_index_arguments'].get('search', 'N/A')}\")\n",
    "        if 'count' in activity_dict:\n",
    "            print(f\"  Results Found: {activity_dict['count']}\")\n",
    "        if 'elapsed_ms' in activity_dict:\n",
    "            print(f\"  Time: {activity_dict['elapsed_ms']}ms\")\n",
    "        print()\n",
    "\n",
    "# Display references to see which content was used\n",
    "if getattr(result_combined, \"references\", None):\n",
    "    print(\"=== References Used ===\")\n",
    "    for i, ref in enumerate(result_combined.references):\n",
    "        ref_dict = ref.as_dict()\n",
    "        print(f\"Reference {i+1}:\")\n",
    "        print(f\"  Document Key: {ref_dict.get('doc_key', 'N/A')}\")\n",
    "        print(f\"  Reranker Score: {ref_dict.get('reranker_score', 'N/A')}\")\n",
    "        print(f\"  Activity Source: {ref_dict.get('activity_source', 'N/A')}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6486c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "references_content:\n",
      " [\n",
      "  {\n",
      "    \"type\": \"searchIndex\",\n",
      "    \"id\": \"0\",\n",
      "    \"activity_source\": 3,\n",
      "    \"reranker_score\": 3.0506465,\n",
      "    \"doc_key\": \"earth_at_night_508_page_104_verbalized\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"searchIndex\",\n",
      "    \"id\": \"1\",\n",
      "    \"activity_source\": 3,\n",
      "    \"reranker_score\": 2.5884187,\n",
      "    \"doc_key\": \"earth_at_night_508_page_105_verbalized\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# References -> JSON string of references as list of dicts\n",
    "if getattr(result, \"references\", None):\n",
    "    references_content = json.dumps([r.as_dict() for r in result.references], indent=2)\n",
    "else:\n",
    "    references_content = \"No references found on 'result'\"\n",
    "    \n",
    "references_contents.append(references_content)\n",
    "print(\"references_content:\\n\", references_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98057c5",
   "metadata": {},
   "source": [
    "## Run an evaluation with Azure AI Foundry\n",
    "\n",
    "To evaluate the groundedness and relevance of the pipeline, run an evaluation with Azure AI Foundry. For more detailed guidance, see [Run evaluations locally by using the Azure AI Foundry SDK](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/evaluate-sdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88117b3",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "+ The same [Azure AI Foundry project](ttps://learn.microsoft.com/azure/ai-foundry/how-to/create-projects) you used for agentic retrieval. Set `FOUNDRY_ENDPOINT` to your project endpoint in the `.env` file. You can find this endpoint on the **Overview** page of your project in the [Azure AI Foundry portal](https://ai.azure.com/).\n",
    "\n",
    "+ The `azure-ai-evaluation` package. Run the following command to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e2a2777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install azure-ai-evaluation --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80001db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing evaluation data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'query_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m evaluation_data = []\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPreparing evaluation data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q, r, g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([query_1, \u001b[43mquery_2\u001b[49m], references_contents, response_contents):\n\u001b[32m     17\u001b[39m     evaluation_data.append({\n\u001b[32m     18\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: q,\n\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m: g,\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: r,\n\u001b[32m     21\u001b[39m     })\n\u001b[32m     23\u001b[39m filename = \u001b[33m\"\u001b[39m\u001b[33mevaluation_data.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'query_2' is not defined"
     ]
    }
   ],
   "source": [
    "# Load connections\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "foundry_endpoint = os.environ[\"FOUNDRY_ENDPOINT\"]\n",
    "aoai_api_version = os.environ[\"AOAI_API_VERSION\"]\n",
    "\n",
    "# Run the evaluation\n",
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration, GroundednessEvaluator, RelevanceEvaluator, evaluate\n",
    "import json\n",
    "\n",
    "evaluation_data = []\n",
    "print(\"Preparing evaluation data...\")\n",
    "for q, r, g in zip([query_1, query_2], references_contents, response_contents):\n",
    "    evaluation_data.append({\n",
    "        \"query\": q,\n",
    "        \"response\": g,\n",
    "        \"context\": r,\n",
    "    })\n",
    "\n",
    "filename = \"evaluation_data.jsonl\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    for item in evaluation_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=aoai_endpoint,\n",
    "    api_version=aoai_api_version,\n",
    "    azure_deployment=aoai_gpt_model\n",
    ")\n",
    "\n",
    "# RAG triad metrics\n",
    "groundedness = GroundednessEvaluator(model_config=model_config)\n",
    "relevance = RelevanceEvaluator(model_config=model_config)\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "result = evaluate(\n",
    "    data=filename,\n",
    "    evaluators={\n",
    "        \"groundedness\": groundedness,\n",
    "        \"relevance\": relevance,\n",
    "    },\n",
    "    azure_ai_project=foundry_endpoint,\n",
    ")\n",
    "\n",
    "print(\"Evaluation complete.\")\n",
    "studio_url = result.get(\"studio_url\")\n",
    "print(\"For more information, go to the Azure AI Foundry portal.\") if studio_url else None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
